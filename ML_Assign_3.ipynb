{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmqvEr_rIn1-",
        "outputId": "2b66e9ed-00b3-4f8b-b5d4-77e99b206842"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import random\n",
        "import nltk\n",
        "import requests\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Download and set stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_tweet(tweet):\n",
        "    # Extract content from tweet\n",
        "    tweet_content = tweet.split('|')[2]\n",
        "\n",
        "    # Remove @mentions, hashtags, URLs, and convert to lowercase\n",
        "    tweet_content = re.sub(r'@\\w+', '', tweet_content)\n",
        "    tweet_content = tweet_content.replace('#', '')\n",
        "    tweet_content = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet_content, flags=re.MULTILINE)\n",
        "    tweet_content = tweet_content.lower()\n",
        "\n",
        "    # Remove stopwords\n",
        "    tweet_content = ' '.join([word for word in tweet_content.split() if word not in stop_words])\n",
        "\n",
        "    return tweet_content\n",
        "\n",
        "# Jaccard Distance Function\n",
        "def jaccard_distance(set1, set2):\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return 1 - intersection / union\n",
        "\n",
        "# K-means Clustering Function\n",
        "def kmeans_clustering(tweets, k):\n",
        "    # Initialize centroids randomly\n",
        "    centroids = random.sample(tweets, k)\n",
        "    clusters = None\n",
        "\n",
        "    while True:\n",
        "        new_clusters = [[] for _ in range(k)]\n",
        "        for tweet in tweets:\n",
        "            distances = [jaccard_distance(set(tweet.split()), set(centroid.split())) for centroid in centroids]\n",
        "            closest_centroid = distances.index(min(distances))\n",
        "            new_clusters[closest_centroid].append(tweet)\n",
        "\n",
        "        new_centroids = []\n",
        "        for cluster in new_clusters:\n",
        "            if len(cluster) == 0:\n",
        "                # Reinitialize centroid if cluster is empty\n",
        "                new_centroids.append(random.choice(tweets))\n",
        "            else:\n",
        "                # Update centroid based on current cluster\n",
        "                new_centroids.append(' '.join(set(word for tweet in cluster for word in tweet.split())))\n",
        "\n",
        "        if new_centroids == centroids:\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "        clusters = new_clusters\n",
        "\n",
        "    return clusters, centroids\n",
        "\n",
        "# Sum of Squared Errors Calculation\n",
        "def calculate_sse(clusters, centroids):\n",
        "    sse = 0\n",
        "    for i, cluster in enumerate(clusters):\n",
        "        centroid_set = set(centroids[i].split())\n",
        "        for tweet in cluster:\n",
        "            sse += jaccard_distance(set(tweet.split()), centroid_set) ** 2\n",
        "    return sse\n",
        "\n",
        "# Load and Preprocess Tweets\n",
        "file_url = 'https://raw.githubusercontent.com/sreeharsha5219/ML_Assignment/main/usnewshealth.txt'\n",
        "response = requests.get(file_url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    tweets = [preprocess_tweet(line.strip()) for line in response.text.split('\\n') if line.strip()]\n",
        "else:\n",
        "    print(\"Failed to retrieve the file. Status code:\", response.status_code)\n",
        "    tweets = []\n",
        "\n",
        "sse_values = []\n",
        "cluster_sizes_all_k = []\n",
        "\n",
        "# Clustering and Results\n",
        "k_values = [5, 10,15,20,25,30]  # Example K values\n",
        "results = []\n",
        "for k in k_values:\n",
        "    clusters, centroids = kmeans_clustering(tweets, k)\n",
        "    sse = calculate_sse(clusters, centroids)\n",
        "    cluster_sizes = [len(cluster) for cluster in clusters]\n",
        "    sse_values.append(sse)\n",
        "    cluster_sizes_all_k.append(cluster_sizes)\n",
        "    results.append({'Value of K': k, 'SSE': sse, 'Size of each cluster': ', '.join(f'{i+1}: {size} tweets' for i, size in enumerate(cluster_sizes))})\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ow"
      ],
      "metadata": {
        "id": "AKxCBmb7ydk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u9Ace2VvIzqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('results.csv')\n"
      ],
      "metadata": {
        "id": "oiHGA2vmKiYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Elbow Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, sse_values, marker='o')\n",
        "plt.title('Elbow Plot for K-Means Clustering')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Sum of Squared Errors (SSE)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Cluster Size Distribution for each k\n",
        "for i, k in enumerate(k_values):\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    plt.bar(range(1, k+1), cluster_sizes_all_k[i])\n",
        "    plt.title(f'Cluster Size Distribution for K = {k}')\n",
        "    plt.xlabel('Cluster Number')\n",
        "    plt.ylabel('Number of Tweets in Cluster')\n",
        "    plt.xticks(range(1, k+1))\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Oj8lNKk1Jn2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DNtRiU6NJuaP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}